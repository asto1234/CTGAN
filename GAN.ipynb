{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cac34-a61e-4dfc-a451-4c0512af6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import pytz as timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d431462-5c19-4d05-96e9-ec2dc0745f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "df = pd.read_csv('real_train_with_time.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f89fb-a7b3-43e1-8b19-2f5dee9622b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime objects\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88433c-a751-4a5d-9d07-4e80e380b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values(by=\"timestamp\")[\"timestamp\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12022b3-e2b0-48cc-8e74-4a4ad219ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get random sample and try different losses - see which loss is the best in data analysis stage\n",
    "\n",
    "\n",
    "### TEMP for testing - comment when finished\n",
    "###df = df.head(100)\n",
    "# Randomly select subset_size rows from the training dataset\n",
    "df = df.sample(n=1000, random_state=42)\n",
    "####\n",
    "\n",
    "# Create an empty DataFrame with the specified columns\n",
    "gan_synth_data = pd.DataFrame(columns=df.columns)\n",
    "# Save the empty DataFrame to a CSV file\n",
    "gan_synth_data.to_csv('gan_synthetic_data_with_time.csv', index=False)\n",
    "\n",
    "# Convert timestamp to datetime objects\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# Ensure that timestamps are timezone-aware\n",
    "df['timestamp'] = df['timestamp'].dt.tz_convert('UTC')\n",
    "# Extract hour, day of week, and month from timestamp\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['day_of_month'] = df['timestamp'].dt.day\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77183bf5-673b-4651-9b90-a89e163f2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# fit transform on all column names except the first one which is the timestamp\n",
    "df.iloc[:, 1:] = scaler.fit_transform(df.iloc[:, 1:])\n",
    "\n",
    "# num of feature for the model architechcture\n",
    "num_features = df.shape[1] - 1\n",
    "\n",
    "# excluding the header\n",
    "num_rows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfced856-aff1-4292-90d6-387d089599ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa4668-e8d2-48ac-9227-709f9379aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf1851-03d5-4171-a96e-78f94ca62a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e91e98-0813-4678-9860-4f8bf692cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d244dcb-be4b-4a67-a1ce-cb28990fc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a45c0a-f516-48d1-9b26-eb2549aee94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features, num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45108a-199b-4e49-982c-4bf67727d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GAN Model - Architechture\n",
    "def build_generator(latent_dim):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(num_rows, input_dim=latent_dim, activation='relu'))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(512, activation='relu'))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(64, activation='relu'))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(32, activation='relu'))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(num_features, activation='sigmoid'))\n",
    "    generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return generator\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(num_rows, input_dim=num_features, activation='relu'))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(Dense(512, activation='relu'))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(Dense(64, activation='relu'))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(Dense(32, activation='relu'))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    # gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00002, beta_1=0.3))\n",
    "    return gan\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, gan, X_train, epochs=4000, batch_size=32):\n",
    "    d_losses_fake = []\n",
    "    d_losses_real = []\n",
    "    g_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Generate random noise as input to the generator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "\n",
    "        # Generate synthetic data with the generator\n",
    "        synthetic_data = generator.predict(noise)\n",
    "\n",
    "        # Select a random batch of real data\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_data = X_train[idx]\n",
    "\n",
    "        # Labels for the generated and real data\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_fake = discriminator.train_on_batch(synthetic_data, fake_labels)\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        # Append losses for plotting\n",
    "        d_losses_fake.append(d_loss_fake)\n",
    "        d_losses_real.append(d_loss_real)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
    "\n",
    "    # Plotting training losses\n",
    "    '''\n",
    "    Ideal Behavior in Training Loss Plots:  \n",
    "    Discriminator Loss (Fake and Real):  \n",
    "    Fake Loss: Initially, the fake loss might be high as the generator is not yet producing realistic data. \n",
    "    It should decrease over time as the generator improves.\n",
    "    Real Loss: The real loss should decrease steadily as the discriminator becomes adept at distinguishing \n",
    "    real data from fake data.\n",
    "    Generator Loss:  \n",
    "    The generator loss should increase over epochs, indicating that the generator is producing more \n",
    "    realistic data that challenges the discriminator.\n",
    "    Indications and Adjustments:  \n",
    "    Overfitting: \n",
    "    Discriminator Loss (Real): If the discriminator loss on real data decreases too quickly and stabilizes\n",
    "    at a very low value, it might indicate overfitting. To address this, you can try adjusting the \n",
    "    capacity of your discriminator or introducing regularization techniques.\n",
    "    Generator Loss: If the generator loss plateaus or stops increasing, it may suggest overfitting or \n",
    "    mode collapse. Adjustments such as modifying the architecture, tweaking learning rates, or \n",
    "    exploring different loss functions may help.\n",
    "    Mode Collapse:\n",
    "    If the generator consistently produces similar samples regardless of the input noise, it's a sign of \n",
    "    mode collapse. You can address this by adjusting the architecture, experimenting with different \n",
    "    optimization strategies, or implementing advanced GAN techniques.\n",
    "    Discriminator Saturation:\n",
    "    If the discriminator loss saturates and stops decreasing, it might indicate that the discriminator is \n",
    "    too powerful. Adjusting the capacity of the discriminator or modifying the learning rates can be helpful.\n",
    "    Fluctuations in Loss: \n",
    "    Some fluctuations in losses are normal, but if they are too erratic or do not stabilize over time, it\n",
    "    might indicate instability in training. Adjusting the learning rates, introducing batch normalization, \n",
    "    or experimenting with different GAN variants could help.\n",
    "        \n",
    "    Discriminator Loss:\n",
    "    The discriminator loss should decrease over epochs. This indicates that the discriminator is getting \n",
    "    better at distinguishing between real and generated data. A decreasing discriminator loss suggests that\n",
    "    the generator is producing data that is becoming more difficult for the discriminator to differentiate\n",
    "    from real data.\n",
    "    Generator Loss:   \n",
    "    The generator loss should increase over epochs. This means that the generator is learning to produce \n",
    "    data that is more realistic and harder for the discriminator to distinguish. An increasing generator \n",
    "    loss is a positive sign that the generator is improving its ability to generate data that resembles \n",
    "    the real data distribution.\n",
    "    It's worth noting that the loss curves might not always follow a smooth trajectory, and some \n",
    "    fluctuation is normal during GAN training. The key is to observe the overall trend over epochs. If \n",
    "    both the discriminator and generator losses are stabilizing or converging, it suggests that the GAN is \n",
    "    learning effectively. If there are issues, such as the generator loss not increasing, it may indicate \n",
    "    challenges in the training process that need to be addressed.\n",
    "    '''\n",
    "    # Plotting training losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(epochs), d_losses_fake, label='Discriminator Loss (Fake Down)')\n",
    "    plt.plot(range(epochs), d_losses_real, label='Discriminator Loss (Real Down)')\n",
    "    plt.plot(range(epochs), g_losses, label='Generator Loss Up')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70912fcf-72d5-49e3-845e-c33ffcf3051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "# Set latent dimension and build models\n",
    "latent_dim = len(df.columns) - 4\n",
    "generator_model = build_generator(latent_dim)\n",
    "discriminator_model = build_discriminator()\n",
    "gan_model = build_gan(generator_model, discriminator_model)\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X_train = df.iloc[:, 1:].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc6376-7658-4d0f-a05c-061b6d1b7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN\n",
    "train_gan(generator_model, discriminator_model, gan_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7b4a4-549a-4656-92de-9d11105fc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gan_model.h5\"\n",
    "# Save the GAN model\n",
    "gan_model.save(model_name)\n",
    "\n",
    "if os.path.exists(model_name):\n",
    "    # To load the saved model later\n",
    "    loaded_gan = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e17b37-8054-4bb1-b3d9-95997174d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Generation\n",
    "def generate_synthetic_data(generator, timestamp):\n",
    "    # Convert the timestamp to the required format\n",
    "    input_timestamp = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Extract hour, day of week, and month from timestamp\n",
    "    hour = input_timestamp.hour\n",
    "    day_of_week = input_timestamp.weekday()\n",
    "    day_of_month = input_timestamp.day\n",
    "    month = input_timestamp.month\n",
    "    # Normalize the extracted features to the range [0, 1]\n",
    "    normalized_hour = hour / 23\n",
    "    normalized_day_of_week = day_of_week / 6\n",
    "    normalized_day_of_month = day_of_month / 29  # change this on the actual n of day in that month -1\n",
    "    normalized_month = month / 11\n",
    "\n",
    "    # Generate synthetic data for the given features\n",
    "    noise = np.random.normal(0, 1, size=(latent_dim - 5,))  # Subtract 5 for hour, day_of_week, month, day_of_month\n",
    "    features = np.array([[normalized_hour, normalized_day_of_week, normalized_day_of_month, normalized_month]])\n",
    "    # Concatenate the normalized features to the noise as conditioning input\n",
    "    noise_with_condition = np.concatenate([noise, features.reshape(-1)], axis=0)\n",
    "    noise_with_condition = np.reshape(noise_with_condition, (1, -1))\n",
    "    noise_with_condition = np.pad(noise_with_condition, ((0, 0), (0, 1)), 'constant')\n",
    "    synthetic_data = generator.predict(noise_with_condition.reshape(1, -1))\n",
    "\n",
    "    # Inverse transform to get the synthetic data in the original scale\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "    print('h w d m')\n",
    "    print(hour, day_of_week, day_of_month, month)\n",
    "    print(np.round(synthetic_data.astype(int))[0][:-5])\n",
    "    # rounding values and keepng as ints\n",
    "    synthetic_data = np.round(synthetic_data.astype(int))\n",
    "    # removing last 4 data points as they do not exist in real data\n",
    "    synthetic_data = synthetic_data[:, :-4]\n",
    "\n",
    "    # Create a DataFrame with the synthetic data but removing the first col timestamp and the last 4 cols with\n",
    "    # feature engineered date and time integers as not retaining coluns for these here\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns[1:-4])\n",
    "\n",
    "    # Set the timestamp column with the provided timestamp\n",
    "    synthetic_df.insert(0, \"timestamp\", input_timestamp)\n",
    "    #synthetic_df['timestamp'] = input_timestamp\n",
    "\n",
    "    return synthetic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba44380-f378-41cf-9a6d-dc5cc6c45ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage 2023-09-10T05:30:00+00:00\n",
    "# # generate synthetic data for each hour from a specific start date - later change to 15 mins ROP instead of 1h\n",
    "# initial_timestamp = '2023-09-08 01:00:00'\n",
    "# timestamp = datetime.strptime(initial_timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "# hours_to_add = 24\n",
    "# number_of_files_per_hour = 4\n",
    "# for i in range(hours_to_add):\n",
    "#     for _ in range(number_of_files_per_hour):\n",
    "#         generated_data = generate_synthetic_data(generator_model, timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#         generated_data.to_csv('gan_synthetic_data_with_time.csv', mode='a', index=False, header=False)\n",
    "#     print(timestamp)\n",
    "#     timestamp += timedelta(hours=1)\n",
    "\n",
    "# ###generated_data = generate_synthetic_data(generator_model, '2023-09-08 19:00:00')\n",
    "\n",
    "# '''\n",
    "# # Read the CSV file\n",
    "# df_original = pd.read_csv('gan_synthetic_data_with_time.csv')\n",
    "# df_original['timestamp'] = pd.to_datetime(df_original['timestamp'])\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate correlation matrix for the generated data\n",
    "# correlation_matrix_generated = df_original.iloc[:, 0:10].corr()\n",
    "\n",
    "# print(correlation_matrix_generated)\n",
    "\n",
    "# # Create heatmap for the generated data\n",
    "# plt.figure(figsize=(24, 10))\n",
    "\n",
    "# # Heatmap for generated data\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.heatmap(correlation_matrix_generated, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix for Generated Data')\n",
    "# '''\n",
    "# # Show the plot\n",
    "# ###plt.show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e1876-1f15-433c-9a76-e2320b06c684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified columns\n",
    "gan_synth_data_2 = pd.DataFrame(columns=df.columns)\n",
    "# Save the empty DataFrame to a CSV file\n",
    "gan_synth_data_2.to_csv('gan_real_synthetic_data_with_time.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50615f7c-fc6d-4850-9d36-11b60777d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_timestamp = '2023-09-08 17:45:00'\n",
    "timestamp = datetime.strptime(initial_timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "hours_to_add = 24*82\n",
    "number_of_files_per_hour = 4\n",
    "for i in range(hours_to_add):\n",
    "    for _ in range(number_of_files_per_hour):\n",
    "        generated_data = generate_synthetic_data(generator_model, timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        generated_data.to_csv('gan_real_synthetic_data_with_time.csv', mode='a', index=False, header=False)\n",
    "    print(timestamp)\n",
    "    timestamp += timedelta(minutes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bb7df-f461-482f-8042-a3d3b67ef80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predicted_values = pd.read_csv(\"gan_real_synthetic_data_with_time.csv\")\n",
    "new_predicted_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e87a6c-722f-4c73-ba78-8d157739d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predicted_values['timestamp'] = pd.to_datetime(new_predicted_values['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2657a-36cc-4a7b-83ee-9e584b3dbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporary = pd.read_csv(\"real_train_with_time.csv\")\n",
    "df_temporary.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92d44f-bcd8-4c9b-ac03-5b611034e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporary = df_temporary.sort_values(by=\"timestamp\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220fcf5f-bec9-44e8-8d80-e87269f150d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporary['timestamp'] = pd.to_datetime(df_temporary['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500e2b4-9abf-44bd-9693-4d7a98a8c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc55ab-675a-4476-b450-aeab4a225b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporary.timestamp[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e38ca-959c-407b-8f73-c8cb857a86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ts = df_temporary[\"timestamp\"].unique()\n",
    "unique_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea86968-bfe1-4045-a259-ee983cc62f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predicted_values.timestamp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa436120-59d3-4b95-b557-50b9e47559ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_new_predicted_df = new_predicted_values[ new_predicted_values['timestamp'].isin(unique_ts) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba265e-012d-4941-89fb-0399f6168aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Df filtered on timestamp columns that present in real_data\n",
    "filter_new_predicted_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cd1a9-c2f9-4acc-ade8-389f95c559db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae71375-c2ab-4d70-a010-5912a85ccbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporary.pmDlAssigsTransPCell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c075f49-2ceb-4a1d-b860-9a852fb58c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_new_predicted_df.pmDlAssigsTransPCell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586455b-6bdf-4178-a237-6c50f12d0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df_temporary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e38bff-eb06-433e-826f-b9e632e8147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.lineplot(df_temporary.timestamp, df_temporary.pmDlAssigsTransPCell[:4500], label=\"real\")\n",
    "#sns.lineplot(filter_new_predicted_df.timestamp, filter_new_predicted_df.pmDlAssigsTransPCell, label=\"predicted\")\n",
    "sns.lineplot(df_temporary.timestamp, filter_new_predicted_df.pmDlAssigsTransPCell, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429f52c-2f27-4015-93c5-fef5eba04c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.lineplot(df_temporary.timestamp, df_temporary.pmCellCcePttReservation_1[:4500], label=\"real\")\n",
    "#sns.lineplot(filter_new_predicted_df.timestamp, filter_new_predicted_df.pmDlAssigsTransPCell, label=\"predicted\")\n",
    "sns.lineplot(df_temporary.timestamp, filter_new_predicted_df.pmCellCcePttReservation_1, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67c3d5-175b-48d2-9f58-3450fd2e3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.lineplot(df_temporary.timestamp, df_temporary.pmPuschSchedActivity[:4500], label=\"real\")\n",
    "#sns.lineplot(filter_new_predicted_df.timestamp, filter_new_predicted_df.pmDlAssigsTransPCell, label=\"predicted\")\n",
    "sns.lineplot(df_temporary.timestamp, filter_new_predicted_df.pmPuschSchedActivity, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b2e65-0a43-4895-a182-2e219c91c181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43758bf-fb8e-4acb-aba5-293d46748ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848c1f7-7d56-402f-b4ae-97f8c9aea5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_real = df_temporary.drop(\"timestamp\", axis=1)\n",
    "# target_pred = filter_new_predicted_df.drop([\"timestamp\", \"hour\", \"day_of_week\", \"day_of_month\", \"month\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0814f-0de2-43bd-a3fe-3922d50cd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_real.shape, target_pred.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce91fb-dbc8-44ec-9698-b1f4d26c48ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common_ts = pd.merge(df_temporary[[\"timestamp\"]], filter_new_predicted_df[[\"timestamp\"]], on=\"timestamp\", how=\"inner\")\n",
    "# common_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b406c5-02a6-4af5-850a-7a5a5d5eb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temporary[\"counter_a\"] = df_temporary.groupby('timestamp').cumcount()\n",
    "# filter_new_predicted_df[\"counter_b\"] = filter_new_predicted_df.groupby('timestamp').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da6832-4c94-43dd-ae90-b0519224cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffc316-3455-4610-9bc0-97a60b259d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2_scores_ = r2_score(target_real, target_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf84bf-d2d5-4b58-8ec3-098749f1414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, score in enumerate(r2_scores_):\n",
    "#     #fit = check_r2_score(score)\n",
    "#     print(f\"R2 score for Target {i+1}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0970ec-9640-4258-bedc-0bcdaaf4e7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
